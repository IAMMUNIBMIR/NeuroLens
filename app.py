import os
import zipfile
import tempfile
from pathlib import Path

import cv2
import numpy as np
import requests
import streamlit as st
import plotly.graph_objects as go
import PIL.Image
import tensorflow as tf
from keras.models import load_model, Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.optimizers import Adamax
from keras.metrics import Precision, Recall
from keras.preprocessing import image
import google.generativeai as genai
from dotenv import load_dotenv

# ---------------------- NEW: tiny constants/helpers --------------------------
LABELS = ['Glioma', 'Meningioma', 'No tumor', 'Pituitary']

SALIENCY_DIR = Path('saliency_maps')
MODELS_DIR   = Path('models')
SALIENCY_DIR.mkdir(parents=True, exist_ok=True)
MODELS_DIR.mkdir(parents=True, exist_ok=True)

def fetch_if_missing(url_env_key: str, dest: Path):
    """Download file from a URL (stored in secrets/env) if it isn't present."""
    if dest.exists():
        return dest
    url = os.getenv(url_env_key)
    if not url:
        raise ValueError(f"{url_env_key} not set in environment/secrets")
    r = requests.get(url, stream=True)
    r.raise_for_status()
    with open(dest, "wb") as f:
        for chunk in r.iter_content(8192):
            f.write(chunk)
    return dest

# ---------------------- ORIGINAL CODE (slightly re-ordered) -------------------
load_dotenv()
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

def generate_explanation(img_path, model_prediction, confidence):
    prompt = f"""You are an expert neurologist. You are tasked with explaining a saliency map of a brain tumor MRI scan. The saliency map was generated by a machine learning model that was trained to classify brain tumors as either glioma, meningioma, pituitary, or no tumor.

    The saliency map highlights the regions of the image that the machine learning model is focusing on to make the prediction.

    The machine learning model predicted the image to of class "{model_prediction}" with a confidence of {confidence * 100}%.

    In your response:
     - Explain what regions of the brain the model is focusing on, based on the saliency map. Refer to the regions highlighted in light cyan, those are the regions where the model is focusing on.
     - Explain possible reasons why the model made the prediction it did.
     - Don't mention anything like "The saliency map highlights the regions the model is focusing on, which are in light cyan" in your explanation.
     - Keep your explanation to 4 sentences max.
    """

    img = PIL.Image.open(img_path)
    model = genai.GenerativeModel(model_name="gemini-1.5-flash")
    response = model.generate_content([prompt, img])
    return response.text


def generate_saliency_map(model, img_array, class_index, img_size):
    with tf.GradientTape() as tape:
        img_tensor = tf.convert_to_tensor(img_array)
        tape.watch(img_tensor)
        predictions = model(img_tensor)
        target_class = predictions[:, class_index]

    gradients = tape.gradient(target_class, img_tensor)
    gradients = tf.math.abs(gradients)
    gradients = tf.reduce_max(gradients, axis=-1)
    gradients = gradients.numpy().squeeze()

    # Resize gradients to match original image size
    gradients = cv2.resize(gradients, img_size)

    # Create a circular mask for the brain area
    center = (gradients.shape[0] // 2, gradients.shape[1] // 2)
    radius = min(center[0], center[1]) - 10
    y, x = np.ogrid[:gradients.shape[0], :gradients.shape[1]]
    mask = (x - center[0])**2 + (y - center[1])**2 <= radius**2

    # Apply mask to gradients
    gradients = gradients * mask

    # Normalize only the brain area
    brain_gradients = gradients[mask]
    if brain_gradients.max() > brain_gradients.min():
        brain_gradients = (brain_gradients - brain_gradients.min()) / (brain_gradients.max() - brain_gradients.min())
    gradients[mask] = brain_gradients

    # Apply a higher threshold
    threshold = np.percentile(gradients[mask], 80)
    gradients[gradients < threshold] = 0

    # Apply more aggressive smoothing
    gradients = cv2.GaussianBlur(gradients, (11, 11), 0)

    return gradients


def load_model_custom(model_path):
    img_shape=(299,299,3)
    base_model = tf.keras.applications.Xception(include_top=False, weights="imagenet",
                                input_shape=img_shape, pooling='max')

    model = Sequential([
        base_model,
        Flatten(),
        Dropout(rate=0.3),
        Dense(128, activation='relu'),
        Dropout(rate=0.25),
        Dense(4, activation='softmax')
    ])

    model.build((None,) + img_shape)

    # Compile the model
    model.compile(Adamax(learning_rate=0.001),
                loss='categorical_crossentropy',
                metrics=['accuracy',
                            Precision(),
                            Recall()])

    model.load_weights(model_path)

    return model

# ---------------------- NEW: DICOM helpers -----------------------------------
def _is_dicom(path: Path) -> bool:
    try:
        with open(path, "rb") as f:
            return f.read(132)[128:132] == b"DICM"
    except Exception:
        return False

def _load_series(folder: Path) -> np.ndarray:
    files = [p for p in folder.rglob("*") if p.is_file() and _is_dicom(p)]
    if not files:
        raise ValueError("No DICOM files found")

    # Group by SeriesInstanceUID and pick the largest series
    import pydicom
    series_map = {}
    for f in files:
        d = pydicom.dcmread(str(f), stop_before_pixels=True, force=True)
        uid = getattr(d, "SeriesInstanceUID", "unknown")
        series_map.setdefault(uid, []).append(f)
    uid, slices = max(series_map.items(), key=lambda kv: len(kv[1]))

    def sort_key(p):
        d = pydicom.dcmread(str(p), stop_before_pixels=True, force=True)
        return getattr(d, "InstanceNumber", getattr(d, "SliceLocation", 0))
    slices = sorted(slices, key=sort_key)

    first = pydicom.dcmread(str(slices[0]))
    intercept = float(getattr(first, "RescaleIntercept", 0))
    slope     = float(getattr(first, "RescaleSlope", 1))

    arrays = []
    for s in slices:
        ds = pydicom.dcmread(str(s))
        arr = ds.pixel_array.astype(np.float32) * slope + intercept
        arrays.append(arr)
    return np.stack(arrays, axis=0)  # (S,H,W)

@st.cache_data(show_spinner=False)
def load_dicom_zip(file_bytes: bytes) -> np.ndarray:
    with tempfile.TemporaryDirectory() as tmpdir:
        tmpdir = Path(tmpdir)
        zpath = tmpdir / "upload.zip"
        with open(zpath, "wb") as f:
            f.write(file_bytes)
        with zipfile.ZipFile(zpath, "r") as zf:
            zf.extractall(tmpdir)
        vol = _load_series(tmpdir)
    return vol

@st.cache_data(show_spinner=False)
def load_single_dcm(file_bytes: bytes) -> np.ndarray:
    import io, pydicom
    ds = pydicom.dcmread(file_bytes)
    arr = ds.pixel_array.astype(np.float32)
    return arr[None, ...]  # (1,H,W)

def normalize_slice(slice2d: np.ndarray) -> np.ndarray:
    s = (slice2d - slice2d.min()) / (slice2d.max() - slice2d.min() + 1e-8)
    return (s * 255).astype("uint8")

def dicom_uploader_and_viewer():
    up = st.file_uploader("Upload MRI DICOM (.zip or .dcm)", type=["zip","dcm"])
    if not up:
        return None, None, None

    ext = up.name.lower().split(".")[-1]
    if ext == "zip":
        vol = load_dicom_zip(up.getvalue())
    else:
        vol = load_single_dcm(up.getvalue())

    st.success(f"Loaded volume shape: {vol.shape}")  # (S,H,W)

    orientation = "axial"  
    default_idx = st.session_state.pop("slice_slider_force", vol.shape[0]//2)
    idx = st.slider("Slice", 0, vol.shape[0]-1, default_idx, 1, key="slice_slider")
    slice_img = normalize_slice(vol[idx])
    st.image(slice_img, caption=f"Axial slice {idx}/{vol.shape[0]-1}", use_container_width=True)
    return vol, orientation, idx

# ---------------------- UI ---------------------------------------------------
st.title("Brain Tumor Classification")
st.write("Upload an image of a brain MRI scan to classify.")

# Model choice FIRST
selected_model = st.radio(
    "Select Model",
    ("Transfer Learning - Xception", "Custom CNN")
)

# Load appropriate model & set image size
if selected_model == "Transfer Learning - Xception":
    model = load_model("xception_full.keras")
    img_size = (299, 299)
else:
    model = load_model('cnn_model.keras')
    img_size = (224, 224)

# ---------------------- NEW: input-type switch -------------------------------
mode = st.radio("Input type", ["Image (PNG/JPG)", "DICOM (.zip/.dcm)"], horizontal=True)

if mode == "DICOM (.zip/.dcm)":
    volume, orientation, slice_idx = dicom_uploader_and_viewer()
    if volume is None:
        st.stop()

    def prep_slice(slice2d, size):
        # resize to model size
        s = cv2.resize(slice2d, size)
        # normalize 0â€“255 for display
        disp = ((s - s.min()) / (s.ptp() + 1e-8) * 255).astype("uint8")
        rgb_disp = np.stack([disp, disp, disp], axis=-1)         # (H,W,3)
        model_in = np.expand_dims(rgb_disp / 255.0, 0).astype("float32")
        return model_in, rgb_disp

    # use the user-selected slice
    img_array, original_img_for_display = prep_slice(volume[slice_idx], img_size)

    run_all = st.checkbox("Analyze all slices", value=False)
    if run_all:
        vol_resized = np.stack([cv2.resize(s, img_size) for s in volume], axis=0)
        vol_rgb = np.repeat(vol_resized[..., None], 3, axis=-1) / 255.0
        preds = model.predict(vol_rgb, batch_size=16)
        top_classes = np.argmax(preds, axis=1)
        no_tumor_idx = LABELS.index("No tumor")  
        tumor_probs = 1 - preds[:, no_tumor_idx]

        st.line_chart(tumor_probs, height=180, use_container_width=True)

        suspicious_idx = int(np.argmax(tumor_probs))
        st.write(f"Most suspicious slice: {suspicious_idx}")
        if st.button("Go to that slice"):
            slice_idx = suspicious_idx
            target_slice = volume[slice_idx]
            res = cv2.resize(target_slice, img_size)
            res = np.stack([res, res, res], axis=-1) / 255.0
            img_array = np.expand_dims(res, axis=0)
            original_img_for_display = res.astype("uint8")

else:
    uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])
    if not uploaded_file:
        st.stop()

    img = image.load_img(uploaded_file, target_size=img_size)
    img_array = image.img_to_array(img)
    original_img_for_display = img_array.copy().astype("uint8")
    img_array = np.expand_dims(img_array, axis=0)
    img_array /= 255.0  # Normalize

# ---------------------- Prediction & Visualization (unchanged) ---------------
prediction = model.predict(img_array)

class_index = np.argmax(prediction[0])
result = LABELS[class_index]

saliency_map = generate_saliency_map(model, img_array, class_index, img_size)

# Create a heatmap overlay with enhanced contrast
heatmap = cv2.applyColorMap(np.uint8(255 * saliency_map), cv2.COLORMAP_JET)
heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)

# Resize heatmap to match original image size
heatmap = cv2.resize(heatmap, img_size)

# Superimpose the heatmap on original image with increased opacity
superimposed_img = heatmap * 0.7 + original_img_for_display * 0.3
superimposed_img = superimposed_img.astype(np.uint8)

img_path = SALIENCY_DIR / (uploaded_file.name if mode != "DICOM (.zip/.dcm)" else "dicom_slice.png")
with open(img_path, "wb") as f:
    if mode != "DICOM (.zip/.dcm)":
        f.write(uploaded_file.getbuffer())
    else:
        # save the slice image
        cv2.imwrite(str(img_path), cv2.cvtColor(original_img_for_display, cv2.COLOR_RGB2BGR))

saliency_map_path = str(SALIENCY_DIR / img_path.name)

# Save the saliency map
cv2.imwrite(saliency_map_path, cv2.cvtColor(superimposed_img, cv2.COLOR_RGB2BGR))

col1, col2 = st.columns(2)
with col1:
    st.image(original_img_for_display, caption='Input Image', use_container_width=True)
with col2:
    st.image(superimposed_img, caption='Saliency Map', use_container_width=True)

# Display the result
st.write("## Classification Results")
result_container = st.container()
result_container.markdown(
     f"""
    <div style="background-color: #000000; color: #ffffff; padding: 30px; border-radius: 15px;">
        <div style="display: flex; justify-content: space-between; align-items: center;">
            <div style="flex: 1; text-align: center;">
                <h3 style="color: #ffffff; margin-bottom: 10px; font-size: 20px;">Prediction</h3>
                <p style="font-size: 36px; font-weight: 800; color: #FF0000; margin: 0;">
                    {result}
                </p>
            </div>
            <div style="width: 2px; height: 80px; background-color: #ffffff; margin: 0 20px;"></div>
            <div style="flex: 1; text-align: center;">
                <h3 style="color: #ffffff; margin-bottom: 10px; font-size: 20px;">Confidence</h3>
                <p style="font-size: 36px; font-weight: 800; color: #2196F3; margin: 0;">
                    {prediction[0][class_index]:.4%}
                </p>
            </div>
        </div>
    </div>
    """,
    unsafe_allow_html=True
)

# Prepare data for Plotly chart
probabilities = prediction[0]
sorted_indices = np.argsort(probabilities)[::-1]
sorted_labels = [LABELS[i] for i in sorted_indices]
sorted_probabilities = probabilities[sorted_indices]

# Create a Plotly bar chart
fig = go.Figure(go.Bar(
    x=sorted_probabilities,
    y=sorted_labels,
    orientation='h',
    marker_color=['red' if label == result else 'blue' for label in sorted_labels]
))

# Customize the chart layout
fig.update_layout(
    title='Probabilities for each class',
    xaxis_title='Probability',
    yaxis_title='Class',
    height=400,
    width=600,
    yaxis=dict(autorange="reversed")
)

# Add value labels to the bars
for i, prob in enumerate(sorted_probabilities):
    fig.add_annotation(
        x=prob,
        y=i,
        text=f'{prob:.4f}',
        showarrow=False,
        xanchor='left',
        xshift=5
    )

# Display the Plotly chart
st.plotly_chart(fig)

explanation = generate_explanation(saliency_map_path, result, prediction[0][class_index])

st.write("## Explanation")
st.write(explanation)
